{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc6aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33dd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your CSV data\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "texts1 = df['Text1'].tolist()\n",
    "texts2 = df['Text2'].tolist()\n",
    "similarities = df['Similarity'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dce20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc009b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Load SpaCy dependency parser\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12061d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarityModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TextSimilarityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3674f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(embeddings, dep_features):\n",
    "    combined = torch.cat((embeddings, dep_features), dim=0)  # Concatenate along rows (dimension 0)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34073b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your model instance\n",
    "input_size = model.get_sentence_embedding_dimension() + 21  # Add size of dependency features\n",
    "similarity_model = TextSimilarityModel(input_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(similarity_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24ac940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\1983001721.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dep_features1 = torch.tensor(dep_features1, dtype=torch.float32)\n",
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\1983001721.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dep_features2 = torch.tensor(dep_features2, dtype=torch.float32)\n",
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\1983001721.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings1 = torch.tensor(embeddings1, dtype=torch.float32)\n",
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\1983001721.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings2 = torch.tensor(embeddings2, dtype=torch.float32)\n",
      "C:\\Users\\prade\\.conda\\envs\\text_proctor\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # You can adjust this\n",
    "for epoch in range(num_epochs):\n",
    "    for text1, text2, similarity in zip(texts1, texts2, similarities):\n",
    "        # Preprocess text and perform dependency parsing with SpaCy\n",
    "        doc1 = nlp(text1)\n",
    "        doc2 = nlp(text2)\n",
    "        \n",
    "        num_unique_relations = 21\n",
    "\n",
    "        # Create a dictionary to map dependency relations to unique indices\n",
    "        dependency_index_map = {'nsubj': 0, 'dobj': 1, 'prep': 2}  # Add more if needed\n",
    "\n",
    "        # Extract relevant features from doc1's dependency parse\n",
    "        dep_features1 = torch.zeros(num_unique_relations)  # Initialize with zeros\n",
    "        for token in doc1:\n",
    "            if token.dep_ in dependency_index_map:  # Check if the dependency relation is relevant\n",
    "                index = dependency_index_map[token.dep_]  # Get the unique index for the relation\n",
    "                dep_features1[index] = 1  # Set the corresponding index to 1\n",
    "\n",
    "        # Extract relevant features from doc2's dependency parse\n",
    "        dep_features2 = torch.zeros(num_unique_relations)  # Initialize with zeros\n",
    "        for token in doc2:\n",
    "            if token.dep_ in dependency_index_map:  # Check if the dependency relation is relevant\n",
    "                index = dependency_index_map[token.dep_]  # Get the unique index for the relation\n",
    "                dep_features2[index] = 1  # Set the corresponding index to 1\n",
    "\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        dep_features1 = torch.tensor(dep_features1, dtype=torch.float32)\n",
    "        dep_features2 = torch.tensor(dep_features2, dtype=torch.float32)\n",
    "\n",
    "        # Convert sentence embeddings to PyTorch tensors\n",
    "        embeddings1 = torch.tensor(embeddings1, dtype=torch.float32)\n",
    "        embeddings2 = torch.tensor(embeddings2, dtype=torch.float32)\n",
    "\n",
    "        # Combine features\n",
    "        combined_features1 = combine_features(embeddings1, dep_features1)\n",
    "        combined_features2 = combine_features(embeddings2, dep_features2)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        output = similarity_model(combined_features1 - combined_features2)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, torch.tensor(similarity, dtype=torch.float32))\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(similarity_model.state_dict(), \"./third_party/fine_tuned_model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "429322b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\3391157992.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dep_features1 = torch.tensor(dep_features1, dtype=torch.float32)\n",
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\3391157992.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dep_features2 = torch.tensor(dep_features2, dtype=torch.float32)\n",
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\3391157992.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings1 = torch.tensor(embeddings1, dtype=torch.float32)\n",
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\3391157992.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings2 = torch.tensor(embeddings2, dtype=torch.float32)\n",
      "C:\\Users\\prade\\.conda\\envs\\text_proctor\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "\n",
    "# Load and preprocess your CSV data\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "texts1 = df['Text1'].tolist()\n",
    "texts2 = df['Text2'].tolist()\n",
    "similarities = df['Similarity'].tolist()\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Load SpaCy dependency parser\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TextSimilarityModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TextSimilarityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def combine_features(embeddings, dep_features):\n",
    "    combined = torch.cat((embeddings, dep_features), dim=0)  # Concatenate along columns (dimension 1)\n",
    "    return combined\n",
    "\n",
    "# Create your model instance\n",
    "input_size = model.get_sentence_embedding_dimension() + 21  # Add size of dependency features\n",
    "similarity_model = TextSimilarityModel(input_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(similarity_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10  # You can adjust this\n",
    "for epoch in range(num_epochs):\n",
    "    for text1, text2, similarity in zip(texts1, texts2, similarities):\n",
    "        # Preprocess text and perform dependency parsing with SpaCy\n",
    "        doc1 = nlp(text1)\n",
    "        doc2 = nlp(text2)\n",
    "        \n",
    "        num_unique_relations = 21\n",
    "\n",
    "        # Create a dictionary to map dependency relations to unique indices\n",
    "        dependency_index_map = {'nsubj': 0, 'dobj': 1, 'prep': 2}  # Add more if needed\n",
    "\n",
    "        # Extract relevant features from doc1's dependency parse\n",
    "        dep_features1 = torch.zeros(num_unique_relations)  # Initialize with zeros\n",
    "        for token in doc1:\n",
    "            if token.dep_ in dependency_index_map:  # Check if the dependency relation is relevant\n",
    "                index = dependency_index_map[token.dep_]  # Get the unique index for the relation\n",
    "                dep_features1[index] = 1  # Set the corresponding index to 1\n",
    "\n",
    "        # Extract relevant features from doc2's dependency parse\n",
    "        dep_features2 = torch.zeros(num_unique_relations)  # Initialize with zeros\n",
    "        for token in doc2:\n",
    "            if token.dep_ in dependency_index_map:  # Check if the dependency relation is relevant\n",
    "                index = dependency_index_map[token.dep_]  # Get the unique index for the relation\n",
    "                dep_features2[index] = 1  # Set the corresponding index to 1\n",
    "\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        dep_features1 = torch.tensor(dep_features1, dtype=torch.float32)\n",
    "        dep_features2 = torch.tensor(dep_features2, dtype=torch.float32)\n",
    "\n",
    "        # Convert sentence embeddings to PyTorch tensors\n",
    "        embeddings1 = torch.tensor(embeddings1, dtype=torch.float32)\n",
    "        embeddings2 = torch.tensor(embeddings2, dtype=torch.float32)\n",
    "\n",
    "        # Combine features\n",
    "        combined_features1 = combine_features(embeddings1, dep_features1)\n",
    "        combined_features2 = combine_features(embeddings2, dep_features2)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        output = similarity_model(combined_features1 - combined_features2)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, torch.tensor(similarity, dtype=torch.float32))\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(similarity_model.state_dict(), \"./third_party/fine_tuned_model_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f25e2a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Similarity: 4.39482307434082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_28492\\995128237.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dep_features = torch.tensor(dep_features, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Load SpaCy dependency parser\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class TextSimilarityModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TextSimilarityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def combine_features(embeddings, dep_features):\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    dep_features = torch.tensor(dep_features, dtype=torch.float32)\n",
    "    combined = torch.cat((embeddings, dep_features), dim=0)  # Concatenate along columns (dimension 1)\n",
    "    return combined\n",
    "\n",
    "# Create your model instance\n",
    "input_size = model.get_sentence_embedding_dimension() + 21  # Add size of dependency features\n",
    "similarity_model = TextSimilarityModel(input_size)\n",
    "\n",
    "# Load the saved model's state dictionary\n",
    "saved_model_path = \"./third_party/fine_tuned_model_2\"\n",
    "similarity_model.load_state_dict(torch.load(saved_model_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "similarity_model.eval()\n",
    "\n",
    "# Example text pairs for testing\n",
    "text1 = \"Recurrent Neural Networks (RNNs) are designed to process sequential data.\"\n",
    "text2 = \"RNNs are used for processing sequential data.\"\n",
    "\n",
    "# Preprocess text and perform dependency parsing with SpaCy\n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "\n",
    "# Create a dictionary to map dependency relations to unique indices\n",
    "dependency_index_map = {'nsubj': 0, 'dobj': 1, 'prep': 2}  # Add more if needed\n",
    "\n",
    "        # Extract relevant features from doc1's dependency parse\n",
    "dep_features1 = torch.zeros(num_unique_relations)  # Initialize with zeros\n",
    "for token in doc1:\n",
    "    if token.dep_ in dependency_index_map:  # Check if the dependency relation is relevant\n",
    "        index = dependency_index_map[token.dep_]  # Get the unique index for the relation\n",
    "        dep_features1[index] = 1  # Set the corresponding index to 1\n",
    "\n",
    "# Extract relevant features from doc2's dependency parse\n",
    "dep_features2 = torch.zeros(num_unique_relations)  # Initialize with zeros\n",
    "for token in doc2:\n",
    "    if token.dep_ in dependency_index_map:  # Check if the dependency relation is relevant\n",
    "        index = dependency_index_map[token.dep_]  # Get the unique index for the relation\n",
    "        dep_features2[index] = 1  # Set the corresponding index to 1\n",
    "\n",
    "\n",
    "# Encode sentence embeddings\n",
    "embeddings1 = model.encode([text1])[0]\n",
    "embeddings2 = model.encode([text2])[0]\n",
    "\n",
    "# Combine features\n",
    "combined_features1 = combine_features(embeddings1, dep_features1)\n",
    "combined_features2 = combine_features(embeddings2, dep_features2)\n",
    "\n",
    "# Forward pass\n",
    "output = similarity_model(combined_features1 - combined_features2)\n",
    "\n",
    "# Print the predicted similarity score\n",
    "print(\"Predicted Similarity:\", output.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
